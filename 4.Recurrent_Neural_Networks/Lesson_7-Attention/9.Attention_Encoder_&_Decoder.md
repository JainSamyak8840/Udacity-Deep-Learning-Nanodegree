In machine translation applications, the encoder and decoder are typically
* Recurrent Neural Networks (Typically vanilla RNN, LSTM, or GRU)

### What's a more reasonable embedding size for a real-world application?
* 200

### What are the steps that require calculating an attention vector in a seq2seq model with attention?
* Every time step in the decoder only